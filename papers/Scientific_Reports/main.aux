\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{naturemag-doi}
\citation{sung2021global}
\citation{louis2021who}
\citation{ostrom2021cbtrus}
\citation{pope2018brain}
\citation{rimmer2017radiologist}
\citation{bruno2015understanding}
\citation{krizhevsky2012imagenet,raghu2019transfusion}
\citation{deepak2019brain,badvza2020classification,swati2019brain,aurna2022multiclass}
\babel@aux{english}{}
\citation{dosovitskiy2021image}
\citation{woo2018cbam}
\citation{chen2018encoder}
\citation{hu2018squeeze}
\citation{sensoy2018evidential}
\citation{woo2018cbam}
\citation{chen2018encoder}
\citation{landis1977measurement}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Per-class classification performance on held-out test set (n = 1,311). CI, confidence interval; AUC-ROC, area under the receiver operating characteristic curve.}}{2}{table.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:main_results}{{1}{2}{Per-class classification performance on held-out test set (n = 1,311). CI, confidence interval; AUC-ROC, area under the receiver operating characteristic curve}{table.caption.1}{}}
\citation{pope2018brain}
\citation{selvaraju2017grad}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Classification performance analysis. (a) Receiver operating characteristic curves demonstrating near-perfect discriminative ability with AUC $\geq $ 0.9999 for all classes. (b) Confusion matrix showing only 3 misclassifications among 1,311 test samples, with all errors involving meningioma as the predicted class.}}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig:roc_confusion}{{1}{3}{Classification performance analysis. (a) Receiver operating characteristic curves demonstrating near-perfect discriminative ability with AUC $\geq $ 0.9999 for all classes. (b) Confusion matrix showing only 3 misclassifications among 1,311 test samples, with all errors involving meningioma as the predicted class}{figure.caption.2}{}}
\citation{deepak2019brain}
\citation{badvza2020classification}
\citation{swati2019brain}
\citation{rehman2020deep}
\citation{aurna2022multiclass}
\citation{kibriya2022novel}
\citation{saeedi2023mri}
\citation{tandel2024multiclass}
\citation{pope2018brain}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Model calibration and interpretability. (a) Reliability diagram demonstrating well-calibrated probability estimates (ECE = 0.019). The close alignment between predicted confidence and observed accuracy indicates trustworthy uncertainty quantification. (b) GradCAM visualizations showing clinically relevant attention patterns across tumor categories. Color scale indicates activation intensity from low (blue) to high (red).}}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:calibration_gradcam}{{2}{4}{Model calibration and interpretability. (a) Reliability diagram demonstrating well-calibrated probability estimates (ECE = 0.019). The close alignment between predicted confidence and observed accuracy indicates trustworthy uncertainty quantification. (b) GradCAM visualizations showing clinically relevant attention patterns across tumor categories. Color scale indicates activation intensity from low (blue) to high (red)}{figure.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Ablation study quantifying component contributions. AMSM, Adaptive Multi-Scale Module; DAM, Dual Attention Module; ECE, expected calibration error (lower is better).}}{4}{table.caption.4}\protected@file@percent }
\newlabel{tab:ablation}{{2}{4}{Ablation study quantifying component contributions. AMSM, Adaptive Multi-Scale Module; DAM, Dual Attention Module; ECE, expected calibration error (lower is better)}{table.caption.4}{}}
\citation{cheng2017figshare}
\citation{cheng2015enhanced,cheng2017retrieval}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparison with published state-of-the-art methods. HSANet addresses the more challenging four-class problem while providing uncertainty quantification and external validation. * indicates results on different dataset splits. $\dagger $ indicates external validation result on independent Figshare dataset.}}{5}{table.caption.5}\protected@file@percent }
\newlabel{tab:comparison}{{3}{5}{Comparison with published state-of-the-art methods. HSANet addresses the more challenging four-class problem while providing uncertainty quantification and external validation. * indicates results on different dataset splits. $\dagger $ indicates external validation result on independent Figshare dataset}{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Detailed analysis of the three misclassified test cases. All errors involved meningioma as the predicted class.}}{5}{table.caption.6}\protected@file@percent }
\newlabel{tab:errors}{{4}{5}{Detailed analysis of the three misclassified test cases. All errors involved meningioma as the predicted class}{table.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Five-fold stratified cross-validation results. Low variance across folds demonstrates robust generalization.}}{5}{table.caption.7}\protected@file@percent }
\newlabel{tab:cv}{{5}{5}{Five-fold stratified cross-validation results. Low variance across folds demonstrates robust generalization}{table.caption.7}{}}
\citation{pan2010survey}
\citation{quinonero2009dataset}
\citation{quinonero2009dataset}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Cross-dataset validation results comparing performance on the original Kaggle test set and the independent Figshare external dataset. The model trained exclusively on Kaggle data achieves excellent generalization to the external dataset despite substantial differences in acquisition protocols and patient demographics.}}{7}{table.caption.8}\protected@file@percent }
\newlabel{tab:cross_validation}{{6}{7}{Cross-dataset validation results comparing performance on the original Kaggle test set and the independent Figshare external dataset. The model trained exclusively on Kaggle data achieves excellent generalization to the external dataset despite substantial differences in acquisition protocols and patient demographics}{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Per-class performance on the external Figshare dataset. All three tumor categories achieve F1-scores exceeding 99.7\%, indicating robust cross-domain generalization.}}{7}{table.caption.9}\protected@file@percent }
\newlabel{tab:cross_class}{{7}{7}{Per-class performance on the external Figshare dataset. All three tumor categories achieve F1-scores exceeding 99.7\%, indicating robust cross-domain generalization}{table.caption.9}{}}
\citation{van2021artificial}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces External validation on Figshare dataset. (a) Confusion matrix showing only 3 misclassifications among 3,064 samples from an independent dataset with different acquisition protocols. (b) Per-class F1-score comparison between original Kaggle and external Figshare datasets, demonstrating consistent performance across domains.}}{8}{figure.caption.10}\protected@file@percent }
\newlabel{fig:cross_validation}{{3}{8}{External validation on Figshare dataset. (a) Confusion matrix showing only 3 misclassifications among 3,064 samples from an independent dataset with different acquisition protocols. (b) Per-class F1-score comparison between original Kaggle and external Figshare datasets, demonstrating consistent performance across domains}{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Uncertainty and calibration analysis for cross-dataset validation. (a) Uncertainty distribution comparison showing consistent uncertainty levels between datasets, with misclassified samples exhibiting elevated uncertainty in both cases. (b) Reliability diagrams demonstrating well-calibrated probability estimates on both original and external datasets (ECE $\leq $ 0.019).}}{8}{figure.caption.11}\protected@file@percent }
\newlabel{fig:uncertainty_external}{{4}{8}{Uncertainty and calibration analysis for cross-dataset validation. (a) Uncertainty distribution comparison showing consistent uncertainty levels between datasets, with misclassified samples exhibiting elevated uncertainty in both cases. (b) Reliability diagrams demonstrating well-calibrated probability estimates on both original and external datasets (ECE $\leq $ 0.019)}{figure.caption.11}{}}
\citation{cheng2017figshare}
\citation{quinonero2009dataset}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Computational efficiency comparison. FLOPs computed for single 224×224 input. Inference time measured on NVIDIA Tesla P100 GPU with batch size 1.}}{9}{table.caption.12}\protected@file@percent }
\newlabel{tab:efficiency}{{8}{9}{Computational efficiency comparison. FLOPs computed for single 224×224 input. Inference time measured on NVIDIA Tesla P100 GPU with batch size 1}{table.caption.12}{}}
\citation{msoud_nickparvar_2021}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Dataset overview showing the four diagnostic categories with sample counts and class distribution. The dataset exhibits mild class imbalance with healthy controls comprising the largest category. The predefined train/test split ensures fair comparison with prior work.}}{11}{figure.caption.13}\protected@file@percent }
\newlabel{fig:dataset_samples}{{5}{11}{Dataset overview showing the four diagnostic categories with sample counts and class distribution. The dataset exhibits mild class imbalance with healthy controls comprising the largest category. The predefined train/test split ensures fair comparison with prior work}{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Representative MRI samples from each diagnostic category. (a) Glioma: irregular enhancing mass with surrounding edema and infiltrative margins. (b) Meningioma: well-circumscribed extra-axial mass with homogeneous enhancement and dural attachment. (c) Pituitary adenoma: sellar/suprasellar mass with characteristic location. (d) Healthy control: normal brain parenchyma without focal abnormality.}}{11}{figure.caption.14}\protected@file@percent }
\newlabel{fig:mri_samples}{{6}{11}{Representative MRI samples from each diagnostic category. (a) Glioma: irregular enhancing mass with surrounding edema and infiltrative margins. (b) Meningioma: well-circumscribed extra-axial mass with homogeneous enhancement and dural attachment. (c) Pituitary adenoma: sellar/suprasellar mass with characteristic location. (d) Healthy control: normal brain parenchyma without focal abnormality}{figure.caption.14}{}}
\citation{tan2019efficientnet}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Data augmentation pipeline applied during training. Each transformation is applied with specified probability to generate diverse training examples while preserving diagnostic content. H-Flip: horizontal flip; Rotate: random rotation; Scale: affine scaling; Jitter: brightness/contrast variation; Erase: random rectangular occlusion.}}{12}{figure.caption.15}\protected@file@percent }
\newlabel{fig:augmentation}{{7}{12}{Data augmentation pipeline applied during training. Each transformation is applied with specified probability to generate diverse training examples while preserving diagnostic content. H-Flip: horizontal flip; Rotate: random rotation; Scale: affine scaling; Jitter: brightness/contrast variation; Erase: random rectangular occlusion}{figure.caption.15}{}}
\citation{woo2018cbam}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Complete HSANet architecture. Input MRI scans (224×224×3) are processed through the EfficientNet-B3 backbone, consisting of 7 stages with MBConv blocks. Features are extracted at three spatial resolutions (28×28, 14×14, 7×7) marked by red dashed arrows. Each feature map passes through the Adaptive Multi-Scale Module (AMSM) for multi-scale processing and Dual Attention Module (DAM) for channel-spatial refinement. Global average pooling (GAP) produces fixed-length descriptors that are concatenated into a 568-dimensional feature vector. The evidential classification head outputs Dirichlet parameters via softplus activation, yielding both class predictions and calibrated uncertainty estimates.}}{13}{figure.caption.16}\protected@file@percent }
\newlabel{fig:architecture}{{8}{13}{Complete HSANet architecture. Input MRI scans (224×224×3) are processed through the EfficientNet-B3 backbone, consisting of 7 stages with MBConv blocks. Features are extracted at three spatial resolutions (28×28, 14×14, 7×7) marked by red dashed arrows. Each feature map passes through the Adaptive Multi-Scale Module (AMSM) for multi-scale processing and Dual Attention Module (DAM) for channel-spatial refinement. Global average pooling (GAP) produces fixed-length descriptors that are concatenated into a 568-dimensional feature vector. The evidential classification head outputs Dirichlet parameters via softplus activation, yielding both class predictions and calibrated uncertainty estimates}{figure.caption.16}{}}
\newlabel{eq:dilated}{{2}{13}{}{equation.2}{}}
\newlabel{eq:fusion}{{3}{13}{}{equation.3}{}}
\newlabel{eq:amsm_output}{{4}{13}{}{equation.4}{}}
\citation{woo2018cbam}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Detailed architecture of the Adaptive Multi-Scale Module (AMSM). Input features $\mathbf  {F}_i$ are processed through three parallel 3×3 dilated convolution branches with dilation rates $r \in \{1, 2, 4\}$, producing effective receptive fields of 3×3, 5×5, and 9×9 respectively. Adaptive fusion weights $\mathbf  {w}_i = [w^{(1)}, w^{(2)}, w^{(4)}]$ are learned through global average pooling (GAP) and a two-layer fully-connected network with softmax normalization. The weighted multi-scale features are combined with a skip connection from the input, enabling the module to learn scale-adaptive refinements. RF: receptive field; BN: batch normalization.}}{14}{figure.caption.17}\protected@file@percent }
\newlabel{fig:amsm}{{9}{14}{Detailed architecture of the Adaptive Multi-Scale Module (AMSM). Input features $\mathbf {F}_i$ are processed through three parallel 3×3 dilated convolution branches with dilation rates $r \in \{1, 2, 4\}$, producing effective receptive fields of 3×3, 5×5, and 9×9 respectively. Adaptive fusion weights $\mathbf {w}_i = [w^{(1)}, w^{(2)}, w^{(4)}]$ are learned through global average pooling (GAP) and a two-layer fully-connected network with softmax normalization. The weighted multi-scale features are combined with a skip connection from the input, enabling the module to learn scale-adaptive refinements. RF: receptive field; BN: batch normalization}{figure.caption.17}{}}
\newlabel{eq:channel_attn}{{5}{14}{}{equation.5}{}}
\newlabel{eq:spatial_attn}{{8}{14}{}{equation.8}{}}
\citation{sensoy2018evidential}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Detailed architecture of the Dual Attention Module (DAM). Channel attention (top) addresses ``what'' features are informative by recalibrating channel responses through parallel global average pooling (GAP) and global max pooling (GMP), followed by a shared two-layer MLP with bottleneck reduction (ratio 16). Spatial attention (bottom) addresses ``where'' to focus by computing channel-wise pooling statistics concatenated and processed through a 7×7 convolution. Sequential application (channel → spatial) enables feature refinement through ``what'' followed by ``where'' reasoning.}}{15}{figure.caption.18}\protected@file@percent }
\newlabel{fig:dam}{{10}{15}{Detailed architecture of the Dual Attention Module (DAM). Channel attention (top) addresses ``what'' features are informative by recalibrating channel responses through parallel global average pooling (GAP) and global max pooling (GMP), followed by a shared two-layer MLP with bottleneck reduction (ratio 16). Spatial attention (bottom) addresses ``where'' to focus by computing channel-wise pooling statistics concatenated and processed through a 7×7 convolution. Sequential application (channel → spatial) enables feature refinement through ``what'' followed by ``where'' reasoning}{figure.caption.18}{}}
\newlabel{eq:alpha}{{12}{15}{}{equation.12}{}}
\citation{lin2017focal}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Evidential deep learning classification head. The 568-dimensional feature vector is projected to 4 logits via a linear layer. Softplus activation plus 1 ensures positive Dirichlet concentration parameters $\boldsymbol  {\alpha } \geq 1$. The Dirichlet distribution over class probabilities yields predictions via its mean and uncertainty via the inverse of total evidence $S$. Bottom: geometric interpretation on the probability simplex---high evidence produces concentrated distributions (confident predictions), low evidence produces spread distributions (uncertain predictions). Uncertainty decomposes into aleatoric (inherent data ambiguity) and epistemic (model knowledge gaps) components.}}{16}{figure.caption.19}\protected@file@percent }
\newlabel{fig:edl}{{11}{16}{Evidential deep learning classification head. The 568-dimensional feature vector is projected to 4 logits via a linear layer. Softplus activation plus 1 ensures positive Dirichlet concentration parameters $\boldsymbol {\alpha } \geq 1$. The Dirichlet distribution over class probabilities yields predictions via its mean and uncertainty via the inverse of total evidence $S$. Bottom: geometric interpretation on the probability simplex---high evidence produces concentrated distributions (confident predictions), low evidence produces spread distributions (uncertain predictions). Uncertainty decomposes into aleatoric (inherent data ambiguity) and epistemic (model knowledge gaps) components}{figure.caption.19}{}}
\citation{loshchilov2017decoupled}
\newlabel{eq:focal}{{19}{17}{}{equation.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Complete training pipeline. Mini-batches undergo data augmentation before forward propagation through HSANet. Three loss terms are computed: evidence-weighted cross-entropy, focal loss for class imbalance, and KL divergence regularization (annealed during early training). AdamW optimizer updates parameters with cosine learning rate annealing. Early stopping monitors validation loss with patience of 7 epochs. Gradient clipping (norm 1.0) ensures training stability.}}{17}{figure.caption.20}\protected@file@percent }
\newlabel{fig:training}{{12}{17}{Complete training pipeline. Mini-batches undergo data augmentation before forward propagation through HSANet. Three loss terms are computed: evidence-weighted cross-entropy, focal loss for class imbalance, and KL divergence regularization (annealed during early training). AdamW optimizer updates parameters with cosine learning rate annealing. Early stopping monitors validation loss with patience of 7 epochs. Gradient clipping (norm 1.0) ensures training stability}{figure.caption.20}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces HSANet Training Procedure}}{18}{algorithm.1}\protected@file@percent }
\newlabel{alg:training}{{1}{18}{HSANet Training Procedure}{algorithm.1}{}}
\citation{selvaraju2017grad}
\bibdata{references}
\bibcite{sung2021global}{1}
\@writefile{toc}{\contentsline {section}{\hspace  *{-\tocsep }References}{19}{equation.24}\protected@file@percent }
\bibcite{louis2021who}{2}
\bibcite{ostrom2021cbtrus}{3}
\bibcite{pope2018brain}{4}
\bibcite{rimmer2017radiologist}{5}
\bibcite{bruno2015understanding}{6}
\bibcite{krizhevsky2012imagenet}{7}
\bibcite{raghu2019transfusion}{8}
\bibcite{deepak2019brain}{9}
\bibcite{badvza2020classification}{10}
\bibcite{swati2019brain}{11}
\bibcite{aurna2022multiclass}{12}
\bibcite{dosovitskiy2021image}{13}
\bibcite{woo2018cbam}{14}
\bibcite{chen2018encoder}{15}
\bibcite{hu2018squeeze}{16}
\bibcite{sensoy2018evidential}{17}
\bibcite{landis1977measurement}{18}
\bibcite{selvaraju2017grad}{19}
\bibcite{rehman2020deep}{20}
\bibcite{kibriya2022novel}{21}
\bibcite{saeedi2023mri}{22}
\bibcite{tandel2024multiclass}{23}
\bibcite{van2021artificial}{24}
\bibcite{msoud_nickparvar_2021}{25}
\bibcite{tan2019efficientnet}{26}
\bibcite{lin2017focal}{27}
\bibcite{loshchilov2017decoupled}{28}
\ttl@finishall
\newlabel{LastPage}{{}{21}{}{page.21}{}}
\gdef\lastpage@lastpage{21}
\gdef\lastpage@lastpageHy{21}
\gdef \@abspage@last{21}
